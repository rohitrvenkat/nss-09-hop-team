{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = Nominatim(user_agent = 'rohit_venkat')\n",
    "\n",
    "\n",
    "def clean_up_address(address):\n",
    "    addr, city, state, zipcode = address[['address_first_line', 'city', 'state', 'zipcode']]\n",
    "    addr = re.split(' APT | BLDG | BOX | BUILDING | FL | RM | STE | SUITE | UNIT | #', addr)[0]\n",
    "    addr = re.sub('1005 D.B. TODD BLVD|1005 DR DB TODD JR BLVD|1005 DR. D. B. TODD BLVD|1005 DR. DB JR BLVD', 'MEHARRY MEDICAL COLLEGE', addr)\n",
    "    addr = re.sub('.*1161 21ST.*', '1161 21ST AVE S', addr)\n",
    "    addr = re.sub('3443 DICKERSON PIKE|3443 DICKERSON PK', 'TRISTAR SKYLINE MEDICAL CENTER', addr)\n",
    "    addr = re.sub('3601 THE VANDERBILT.*|.*VANDERBILT CLINIC.*|.*TVC.*', 'THE VANDERBILT CLINIC', addr)\n",
    "    addr = re.sub('.*OXFORD HOUSE.*', '1313 21ST AVE S', addr)\n",
    "    addr = re.sub('.*MEDICAL CENTER NORTH.*|.*MEDICAL CTR N.*|.*MCN.*', '1161 21ST AVE S', addr)\n",
    "    addr = re.sub('.*MEDICAL CENTER EAST.*|.*MED CTR E.*|.*MCE.*', '1215 21ST AVE S', addr)\n",
    "    addr = re.sub('VANDERBILT CHILDREN.*', '2200 CHILDRENS WAY', addr)\n",
    "    city = re.sub('NASHVILE|NASVHILLE', 'NASHVILLE', city)\n",
    "\n",
    "    return addr + ', ' + city + ', ' + state + ', ' + zipcode\n",
    "\n",
    "\n",
    "def geocode_lookup(address):\n",
    "    try:\n",
    "        time.sleep(0.1)\n",
    "        location = locator.geocode(address)\n",
    "\n",
    "        try:\n",
    "            zipcode = re.findall('Tennessee, ([0-9]{5})', location.address)[0]\n",
    "            return location.address, zipcode, location.latitude, location.longitude\n",
    "\n",
    "        except:\n",
    "            return location.address, None, location.latitude, location.longitude\n",
    "            \n",
    "    except:\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_providers = pd.read_csv('../data/tn_providers.csv', dtype = str)\n",
    "hop_teaming = pd.read_csv('../data/tn_hop_teaming.csv', dtype = {'from_npi': str, 'to_npi': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addresses: 11,407; Matches: 9,465; Runtime: 253m 23.6s\n",
    "tn_team_hopping_addresses = tn_providers[tn_providers['npi'].isin(hop_teaming['from_npi'].append(hop_teaming['to_npi']).reset_index(drop = True))]\n",
    "tn_team_hopping_addresses = tn_team_hopping_addresses[['address_first_line', 'address_second_line', 'city', 'state', 'zipcode']].drop_duplicates()\n",
    "tn_team_hopping_addresses = tn_team_hopping_addresses.assign(geo_query = tn_team_hopping_addresses.apply(clean_up_address, axis = 1))\n",
    "tn_team_hopping_addresses[['geo_address', 'geo_zipcode', 'lat', 'long']] = pd.DataFrame(tn_team_hopping_addresses['geo_query'].apply(geocode_lookup).tolist(), index = tn_team_hopping_addresses.index)\n",
    "tn_team_hopping_addresses.to_csv('../data/tn_team_hopping_addresses.csv', index = False)\n",
    "\n",
    "mismatched = tn_team_hopping_addresses[(~tn_team_hopping_addresses['geo_address'].isnull()) & (tn_team_hopping_addresses['geo_zipcode'] != tn_team_hopping_addresses['zipcode'])]\n",
    "mismatched.to_csv('../data/tn_team_hopping_mismatched_addresses.csv', index = False)\n",
    "\n",
    "unmatched = tn_team_hopping_addresses[tn_team_hopping_addresses['geo_address'].isnull()]\n",
    "unmatched.to_csv('../data/tn_team_hopping_unmatched_addresses.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addresses: 8,771; Matches: 6,651 Runtime: 120m 11.1s\n",
    "tn_provider_addresses = tn_providers[tn_providers['npi'].isin(hop_teaming['from_npi'])]\n",
    "tn_provider_addresses = tn_provider_addresses[['address_first_line', 'address_second_line', 'city', 'state', 'zipcode']].drop_duplicates()\n",
    "tn_provider_addresses = tn_provider_addresses.assign(geo_query = tn_provider_addresses.apply(clean_up_address, axis = 1))\n",
    "tn_provider_addresses[['geo_address', 'geo_zipcode', 'lat', 'long']] = pd.DataFrame(tn_team_hopping_addresses['geo_query'].apply(geocode_lookup).tolist(), index = tn_team_hopping_addresses.index)\n",
    "tn_provider_addresses.to_csv('../data/tn_provider_addresses.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addresses: 4,822; Matches: 4,039; Runtime: 107m 21.1s\n",
    "tn_facility_addresses = tn_providers[tn_providers['npi'].isin(hop_teaming['to_npi'])]\n",
    "tn_facility_addresses = tn_facility_addresses[['address_first_line', 'address_second_line', 'city', 'state', 'zipcode']].drop_duplicates()\n",
    "tn_facility_addresses = tn_facility_addresses.assign(geo_query = tn_facility_addresses.apply(clean_up_address, axis = 1))\n",
    "tn_facility_addresses[['geo_address', 'geo_zipcode', 'lat', 'long']] = pd.DataFrame(tn_facility_addresses['geo_query'].apply(geocode_lookup).tolist(), index = tn_facility_addresses.index)\n",
    "tn_facility_addresses.to_csv('../data/tn_facility_addresses.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addresses: 1,155; Matches: 973; Runtime: 11m 10.3s\n",
    "nashville_facility_addresses = tn_providers[(tn_providers['npi'].isin(hop_teaming['to_npi'])) & (tn_providers['cbsa'] == '34980')]\n",
    "nashville_facility_addresses = nashville_facility_addresses[['address_first_line', 'address_second_line', 'city', 'state', 'zipcode']].drop_duplicates()\n",
    "nashville_facility_addresses = nashville_facility_addresses.assign(geo_query = nashville_facility_addresses.apply(clean_up_address, axis = 1))\n",
    "nashville_facility_addresses[['geo_address', 'geo_zipcode', 'lat', 'long']] = pd.DataFrame(nashville_facility_addresses['geo_query'].apply(geocode_lookup).tolist(), index = nashville_facility_addresses.index)\n",
    "nashville_facility_addresses.to_csv('../data/nashville_facility_addresses.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d6140ef0c675026b0200147df87972487ebc0097827c4c765c9e0dcd9cf7b2f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
